{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23f24f5d-223c-47f7-89dc-3fc746f162a5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/26 08:53:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/03/26 08:53:21 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"YourAppName\")\n",
    "    .config(\"spark.master\", \"local[*]\")\n",
    "    .config(\"spark.driver.extraClassPath\", \"/spark/jars/sqlite-jdbc-3.36.0.3.jar\")\n",
    "    .config(\"spark.jars\", \"/spark/jars/sqlite-jdbc-3.36.0.3.jar\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe8cfcf-5ac2-45e8-a958-eb3e2467b577",
   "metadata": {},
   "source": [
    "## CSV 파일"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de49feb-d0bc-4b7d-962f-4387d1b3a6f0",
   "metadata": {},
   "source": [
    "### CSV 파일 읽기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05ef3069-3f90-4ac7-8713-448f88539bbf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|    1|\n",
      "|    United States|            Ireland|  264|\n",
      "|    United States|              India|   69|\n",
      "|            Egypt|      United States|   24|\n",
      "|Equatorial Guinea|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import LongType, StringType, StructField, StructType\n",
    "\n",
    "# 스키마 정의\n",
    "myManualSchema = StructType(\n",
    "    [\n",
    "        StructField(\"DEST_COUNTRY_NAME\", StringType(), True),\n",
    "        StructField(\"ORIGIN_COUNTRY_NAME\", StringType(), True),\n",
    "        StructField(\"count\", LongType(), False),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# CSV 파일 읽기\n",
    "df = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"mode\", \"FAILFAST\")\n",
    "    .schema(myManualSchema)\n",
    "    .load(\"/data/flight-data/csv/2010-summary.csv\")\n",
    ")\n",
    "\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd9b766d-81a7-4ba4-b34a-4fffa72c5057",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/26 08:53:33 ERROR Executor: Exception in task 0.0 in stage 1.0 (TID 1)\n",
      "org.apache.spark.SparkException: Encountered error while reading file file:///data/flight-data/csv/2010-summary.csv. Details:\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFilesError(QueryExecutionErrors.scala:877)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:307)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:125)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.SparkException: [MALFORMED_RECORD_IN_PARSING] Malformed records are detected in record parsing: [United States,null,1].\n",
      "Parse Mode: FAILFAST. To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedRecordsDetectedInRecordParsingError(QueryExecutionErrors.scala:1772)\n",
      "\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:74)\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:456)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:125)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:297)\n",
      "\t... 20 more\n",
      "Caused by: org.apache.spark.sql.catalyst.util.BadRecordException: java.lang.NumberFormatException: For input string: \"Romania\"\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:365)\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$parse$2(UnivocityParser.scala:307)\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$1(UnivocityParser.scala:452)\n",
      "\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:60)\n",
      "\t... 26 more\n",
      "Caused by: java.lang.NumberFormatException: For input string: \"Romania\"\n",
      "\tat java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)\n",
      "\tat java.lang.Long.parseLong(Long.java:589)\n",
      "\tat java.lang.Long.parseLong(Long.java:631)\n",
      "\tat scala.collection.immutable.StringLike.toLong(StringLike.scala:309)\n",
      "\tat scala.collection.immutable.StringLike.toLong$(StringLike.scala:309)\n",
      "\tat scala.collection.immutable.StringOps.toLong(StringOps.scala:33)\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$8(UnivocityParser.scala:192)\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$8$adapted(UnivocityParser.scala:192)\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.nullSafeDatum(UnivocityParser.scala:291)\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$7(UnivocityParser.scala:192)\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:346)\n",
      "\t... 29 more\n",
      "24/03/26 08:53:33 WARN TaskSetManager: Lost task 0.0 in stage 1.0 (TID 1) (7e98ef8cae67 executor driver): org.apache.spark.SparkException: Encountered error while reading file file:///data/flight-data/csv/2010-summary.csv. Details:\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFilesError(QueryExecutionErrors.scala:877)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:307)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:125)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.SparkException: [MALFORMED_RECORD_IN_PARSING] Malformed records are detected in record parsing: [United States,null,1].\n",
      "Parse Mode: FAILFAST. To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedRecordsDetectedInRecordParsingError(QueryExecutionErrors.scala:1772)\n",
      "\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:74)\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:456)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:125)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:297)\n",
      "\t... 20 more\n",
      "Caused by: org.apache.spark.sql.catalyst.util.BadRecordException: java.lang.NumberFormatException: For input string: \"Romania\"\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:365)\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$parse$2(UnivocityParser.scala:307)\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$1(UnivocityParser.scala:452)\n",
      "\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:60)\n",
      "\t... 26 more\n",
      "Caused by: java.lang.NumberFormatException: For input string: \"Romania\"\n",
      "\tat java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)\n",
      "\tat java.lang.Long.parseLong(Long.java:589)\n",
      "\tat java.lang.Long.parseLong(Long.java:631)\n",
      "\tat scala.collection.immutable.StringLike.toLong(StringLike.scala:309)\n",
      "\tat scala.collection.immutable.StringLike.toLong$(StringLike.scala:309)\n",
      "\tat scala.collection.immutable.StringOps.toLong(StringOps.scala:33)\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$8(UnivocityParser.scala:192)\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$8$adapted(UnivocityParser.scala:192)\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.nullSafeDatum(UnivocityParser.scala:291)\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$7(UnivocityParser.scala:192)\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:346)\n",
      "\t... 29 more\n",
      "\n",
      "24/03/26 08:53:33 ERROR TaskSetManager: Task 0 in stage 1.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o54.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1) (7e98ef8cae67 executor driver): org.apache.spark.SparkException: Encountered error while reading file file:///data/flight-data/csv/2010-summary.csv. Details:\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFilesError(QueryExecutionErrors.scala:877)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:307)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:125)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.SparkException: [MALFORMED_RECORD_IN_PARSING] Malformed records are detected in record parsing: [United States,null,1].\nParse Mode: FAILFAST. To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedRecordsDetectedInRecordParsingError(QueryExecutionErrors.scala:1772)\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:74)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:456)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:125)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:297)\n\t... 20 more\nCaused by: org.apache.spark.sql.catalyst.util.BadRecordException: java.lang.NumberFormatException: For input string: \"Romania\"\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:365)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$parse$2(UnivocityParser.scala:307)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$1(UnivocityParser.scala:452)\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:60)\n\t... 26 more\nCaused by: java.lang.NumberFormatException: For input string: \"Romania\"\n\tat java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)\n\tat java.lang.Long.parseLong(Long.java:589)\n\tat java.lang.Long.parseLong(Long.java:631)\n\tat scala.collection.immutable.StringLike.toLong(StringLike.scala:309)\n\tat scala.collection.immutable.StringLike.toLong$(StringLike.scala:309)\n\tat scala.collection.immutable.StringOps.toLong(StringOps.scala:33)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$8(UnivocityParser.scala:192)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$8$adapted(UnivocityParser.scala:192)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.nullSafeDatum(UnivocityParser.scala:291)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$7(UnivocityParser.scala:192)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:346)\n\t... 29 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4216)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3200)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4206)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:526)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4204)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4204)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3200)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3421)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:283)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:322)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.SparkException: Encountered error while reading file file:///data/flight-data/csv/2010-summary.csv. Details:\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFilesError(QueryExecutionErrors.scala:877)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:307)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:125)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: org.apache.spark.SparkException: [MALFORMED_RECORD_IN_PARSING] Malformed records are detected in record parsing: [United States,null,1].\nParse Mode: FAILFAST. To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedRecordsDetectedInRecordParsingError(QueryExecutionErrors.scala:1772)\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:74)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:456)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:125)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:297)\n\t... 20 more\nCaused by: org.apache.spark.sql.catalyst.util.BadRecordException: java.lang.NumberFormatException: For input string: \"Romania\"\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:365)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$parse$2(UnivocityParser.scala:307)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$1(UnivocityParser.scala:452)\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:60)\n\t... 26 more\nCaused by: java.lang.NumberFormatException: For input string: \"Romania\"\n\tat java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)\n\tat java.lang.Long.parseLong(Long.java:589)\n\tat java.lang.Long.parseLong(Long.java:631)\n\tat scala.collection.immutable.StringLike.toLong(StringLike.scala:309)\n\tat scala.collection.immutable.StringLike.toLong$(StringLike.scala:309)\n\tat scala.collection.immutable.StringOps.toLong(StringOps.scala:33)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$8(UnivocityParser.scala:192)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$8$adapted(UnivocityParser.scala:192)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.nullSafeDatum(UnivocityParser.scala:291)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$7(UnivocityParser.scala:192)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:346)\n\t... 29 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 17\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# CSV 파일 읽기 -> 스파크 잡은 시작하자마자 종료된다. [지연 연산 특성]\u001b[39;00m\n\u001b[1;32m     11\u001b[0m df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsv\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheader\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFAILFAST\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;241m.\u001b[39mschema(myManualSchema) \\\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/data/flight-data/csv/2010-summary.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 17\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/dataframe.py:899\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m    894\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    895\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m    896\u001b[0m     )\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[0;32m--> 899\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    900\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    901\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py:169\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    171\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o54.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1) (7e98ef8cae67 executor driver): org.apache.spark.SparkException: Encountered error while reading file file:///data/flight-data/csv/2010-summary.csv. Details:\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFilesError(QueryExecutionErrors.scala:877)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:307)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:125)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.SparkException: [MALFORMED_RECORD_IN_PARSING] Malformed records are detected in record parsing: [United States,null,1].\nParse Mode: FAILFAST. To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedRecordsDetectedInRecordParsingError(QueryExecutionErrors.scala:1772)\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:74)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:456)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:125)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:297)\n\t... 20 more\nCaused by: org.apache.spark.sql.catalyst.util.BadRecordException: java.lang.NumberFormatException: For input string: \"Romania\"\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:365)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$parse$2(UnivocityParser.scala:307)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$1(UnivocityParser.scala:452)\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:60)\n\t... 26 more\nCaused by: java.lang.NumberFormatException: For input string: \"Romania\"\n\tat java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)\n\tat java.lang.Long.parseLong(Long.java:589)\n\tat java.lang.Long.parseLong(Long.java:631)\n\tat scala.collection.immutable.StringLike.toLong(StringLike.scala:309)\n\tat scala.collection.immutable.StringLike.toLong$(StringLike.scala:309)\n\tat scala.collection.immutable.StringOps.toLong(StringOps.scala:33)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$8(UnivocityParser.scala:192)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$8$adapted(UnivocityParser.scala:192)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.nullSafeDatum(UnivocityParser.scala:291)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$7(UnivocityParser.scala:192)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:346)\n\t... 29 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4216)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3200)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4206)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:526)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4204)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4204)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3200)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3421)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:283)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:322)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.SparkException: Encountered error while reading file file:///data/flight-data/csv/2010-summary.csv. Details:\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFilesError(QueryExecutionErrors.scala:877)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:307)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:125)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: org.apache.spark.SparkException: [MALFORMED_RECORD_IN_PARSING] Malformed records are detected in record parsing: [United States,null,1].\nParse Mode: FAILFAST. To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedRecordsDetectedInRecordParsingError(QueryExecutionErrors.scala:1772)\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:74)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:456)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:125)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:297)\n\t... 20 more\nCaused by: org.apache.spark.sql.catalyst.util.BadRecordException: java.lang.NumberFormatException: For input string: \"Romania\"\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:365)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$parse$2(UnivocityParser.scala:307)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$1(UnivocityParser.scala:452)\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:60)\n\t... 26 more\nCaused by: java.lang.NumberFormatException: For input string: \"Romania\"\n\tat java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)\n\tat java.lang.Long.parseLong(Long.java:589)\n\tat java.lang.Long.parseLong(Long.java:631)\n\tat scala.collection.immutable.StringLike.toLong(StringLike.scala:309)\n\tat scala.collection.immutable.StringLike.toLong$(StringLike.scala:309)\n\tat scala.collection.immutable.StringOps.toLong(StringOps.scala:33)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$8(UnivocityParser.scala:192)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$8$adapted(UnivocityParser.scala:192)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.nullSafeDatum(UnivocityParser.scala:291)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$7(UnivocityParser.scala:192)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:346)\n\t... 29 more\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import LongType, StringType, StructField, StructType\n",
    "\n",
    "# 스키마 정의\n",
    "myManualSchema = StructType(\n",
    "    [\n",
    "        StructField(\"DEST_COUNTRY_NAME\", StringType(), True),\n",
    "        StructField(\"ORIGIN_COUNTRY_NAME\", LongType(), True),  # 잘못된 데이터 포맷 설정\n",
    "        StructField(\"count\", LongType(), False),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# CSV 파일 읽기 -> 스파크 잡은 시작하자마자 종료된다. [지연 연산 특성]\n",
    "df = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"mode\", \"FAILFAST\")\n",
    "    .schema(myManualSchema)\n",
    "    .load(\"/data/flight-data/csv/2010-summary.csv\")\n",
    ")\n",
    "\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d724daad-916a-400f-8bab-de89e9090069",
   "metadata": {},
   "source": [
    "### CSV 파일 쓰기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9069d42-3934-4992-9176-6b373621ac92",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "csvFile = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"mode\", \"FAILFAST\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .load(\"/data/flight-data/csv/2010-summary.csv\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2129267-0746-436c-834c-d50542a4dc3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "csvFile.write.format(\"csv\").mode(\"overwrite\").option(\"sep\", \"\\t\").save(\n",
    "    \"/tmp/my-tsv-file.tsv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd00eb43-2dc1-467f-84c6-f441b11c424b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "part-00000-0ac772ec-d9c1-446c-8a8f-777e7faa4fe4-c000.csv  _SUCCESS\n"
     ]
    }
   ],
   "source": [
    "%ls /tmp/my-tsv-file.tsv/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121dbe89-ba4c-4af6-a7ae-91b700ba4eb7",
   "metadata": {},
   "source": [
    "## JSON 파일"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0987b0-42b9-488a-a5c3-36a491d2c620",
   "metadata": {},
   "source": [
    "### JSON 파일 읽기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe6fa014-3d03-40e6-bf54-9b8f5a0491a6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|    1|\n",
      "|    United States|            Ireland|  264|\n",
      "|    United States|              India|   69|\n",
      "|            Egypt|      United States|   24|\n",
      "|Equatorial Guinea|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.format(\"json\").option(\"mode\", \"FAILFAST\").option(\"inferSchema\", \"true\").load(\n",
    "    \"/data/flight-data/json/2010-summary.json\"\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23cf784a-4990-4823-9bf1-a4c0b87dd204",
   "metadata": {},
   "source": [
    "### JSON 파일 쓰기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "031c8425-ac36-45d6-bc36-1502afe0a9b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "csvFile.write.format(\"json\").mode(\"overwrite\").save(\"/tmp/my-json-file.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "895bf62c-699e-4428-ad45-120a90d275bf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "part-00000-89ece51c-ae79-4c2a-b7d6-0a2993757640-c000.json  _SUCCESS\n"
     ]
    }
   ],
   "source": [
    "%ls /tmp/my-json-file.json/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2350e762-6ab8-453d-986c-ccafb8bd6fe7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 파케이 파일\n",
    "\n",
    "- 파케이\n",
    "    - 다양한 스토리지 최적화 기술을 제공하는 오픈소스로 만들어진 컬럼 기반의 데이터 저장 방식\n",
    "    - 분석 워크로드에 최적화\n",
    "    - 저장소 공간을 절약할 수 있고, 전체 파일 대신 개별 컬럼을 읽을 수 있음\n",
    "    - 컬럼 기반의 압축 기능 제공\n",
    "    - 스파크와 잘 호환되기 때문에 스파크의 기본 파일 포맷이기도 함\n",
    "    - 읽기 연산 시 JSON이나 CSV보다 훨씬 효율적으로 동작하므로 장기 저장용 데이터는 파케이 포맷으로 저장하는 것이 좋음\n",
    "    - 복합 데이터 타입 지원 (배열, 맵, 구조체, ...) <- CSV에서는 배열 사용 불가"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5216dd9d-d063-47e2-9cda-08b895e7c8a4",
   "metadata": {},
   "source": [
    "### 파케이 파일 읽기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "010d89fb-357b-4fae-9654-8328da7f8494",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|    1|\n",
      "|    United States|            Ireland|  264|\n",
      "|    United States|              India|   69|\n",
      "|            Egypt|      United States|   24|\n",
      "|Equatorial Guinea|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.format(\"parquet\").load(\n",
    "    \"/data/flight-data/parquet/2010-summary.parquet\"\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982b6f31-3019-47d5-a64e-b80d0a41e31b",
   "metadata": {},
   "source": [
    "### 파케이 파일 쓰기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52b03891-0914-4a3d-8b88-e7173feb2fe3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "csvFile.write.format(\"parquet\").mode(\"overwrite\").save(\"/tmp/my-parquet-file.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "73e397ba-b3ca-4402-ae0f-9ff76b390aa7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "part-00000-85131749-758a-42f8-858f-882840db18c0-c000.snappy.parquet  _SUCCESS\n"
     ]
    }
   ],
   "source": [
    "%ls /tmp/my-parquet-file.parquet/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35caffac-30b1-4fad-81f6-4d232c39dc46",
   "metadata": {
    "tags": []
   },
   "source": [
    "## ORC 파일\n",
    "\n",
    "- ORC\n",
    "    - 하둡 워크로드를 위해 설계된 자기 기술적 \\[self-describing\\]\n",
    "    - 데이터 타입을 인식할 수 있는 컬럼 기반의 파일 포맷\n",
    "    - 대규모 스트리밍 읽기에 최적화\n",
    "    - 필요한 로우를 신속하게 찾아낼 수 있는 기능이 통합되어 있음\n",
    "    - 파케이와 매우 유사하나, 파케이는 스파크에 최적화 되어 있고 ORC는 Hive에 최적화 되어 있다는 차이가 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3274eca5-8cba-4a41-b804-3b5262c993c2",
   "metadata": {},
   "source": [
    "### ORC 파일 읽기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "37d44136-3dc5-4cdf-85ba-34bcd4c75adf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|    1|\n",
      "|    United States|            Ireland|  264|\n",
      "|    United States|              India|   69|\n",
      "|            Egypt|      United States|   24|\n",
      "|Equatorial Guinea|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.format(\"orc\").load(\"/data/flight-data/orc/2010-summary.orc\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899cf2b6-6000-4e24-9bac-0ea38bc2479c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### ORC 파일 쓰기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "40c359ae-a106-4ef0-be22-15417b5fdd12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "csvFile.write.format(\"orc\").mode(\"overwrite\").save(\"/tmp/my-orc-file.orc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d5446575-47f2-466a-a9d0-ac2590043a01",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "part-00000-57b5b200-6beb-4e34-93af-9bbcc3031e44-c000.snappy.orc  _SUCCESS\n"
     ]
    }
   ],
   "source": [
    "%ls /tmp/my-orc-file.orc/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccda475-f6c0-4ff5-9fa5-7afae505aafa",
   "metadata": {},
   "source": [
    "## SQL 데이터베이스"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05efefee-a8c7-4aaa-ba66-15f708d3a4bb",
   "metadata": {},
   "source": [
    "- SQLite\n",
    "    - 강력하고 빠르며 이해하기 쉬움\n",
    "    - 설치와 실행이 쉬움\n",
    "    - 데이터베이스 접속 시 사용자 인증이 필요 없음\n",
    "    - 쓰기 연산 시 전체 데이터베이스에 락을 설정하므로 분산 환경에 적합하지 않다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9df7906b-a452-43b4-ba59-f8ab7e70beb1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "driver = \"org.sqlite.JDBC\"\n",
    "path = \"/data/flight-data/jdbc/my-sqlite.db\"\n",
    "url = \"jdbc:sqlite:\" + path\n",
    "tablename = \"flight_info\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e3e73e-486c-4bdf-83af-61ea86557274",
   "metadata": {},
   "source": [
    "### [SQLite] SQL 테이블 읽어서 DataFrame 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "50ae4bb4-716d-487b-a6f9-8947bb4d883f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dbDataFrame = (\n",
    "    spark.read.format(\"jdbc\")\n",
    "    .option(\"url\", url)\n",
    "    .option(\"dbtable\", tablename)\n",
    "    .option(\"driver\", driver)\n",
    "    .load()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c9743e-9dce-4ed5-ab8c-ed41a35aaea8",
   "metadata": {
    "tags": []
   },
   "source": [
    "오류가 날 경우 `sqlite-jdbc` jar 파일을 설치한 후 스파크 세션을 시작할 때 config 옵션으로 해당 jdbc를 지정해야 한다.  \n",
    "wget https://github.com/xerial/sqlite-jdbc/releases/download/3.36.0.3/sqlite-jdbc-3.36.0.3.jar -O /$SPARK_HOME/jars/sqlite-jdbc-3.36.0.3.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "17ae0abe-7d67-4771-9326-92a83b7d6b00",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|DEST_COUNTRY_NAME|\n",
      "+-----------------+\n",
      "|         Anguilla|\n",
      "|           Russia|\n",
      "|         Paraguay|\n",
      "|          Senegal|\n",
      "|           Sweden|\n",
      "+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dbDataFrame.select(\"DEST_COUNTRY_NAME\").distinct().show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246b1d34-3eec-4ca8-b8ef-046769d61457",
   "metadata": {},
   "source": [
    "### 쿼리 푸시다운  \n",
    "  \n",
    "  DataFrame을 만들기 전에 데이터베이스 자체에서 데이터를 필터링하도록 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0a9a6ba4-af47-466f-b2d0-f321da6481ec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Scan JDBCRelation(flight_info) [numPartitions=1] [DEST_COUNTRY_NAME#169,ORIGIN_COUNTRY_NAME#170,count#171] PushedFilters: [*In(DEST_COUNTRY_NAME, [Anguilla,Sweden])], ReadSchema: struct<DEST_COUNTRY_NAME:string,ORIGIN_COUNTRY_NAME:string,count:decimal(20,0)>\n"
     ]
    }
   ],
   "source": [
    "# 해당 필터에 대한 처리를 데이터베이스로 위임\n",
    "dbDataFrame.filter(\"DEST_COUNTRY_NAME in ('Anguilla', 'Sweden')\").explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1b517619-f071-461e-99f9-844af2eb49e7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|DEST_COUNTRY_NAME|\n",
      "+-----------------+\n",
      "|    United States|\n",
      "|            Egypt|\n",
      "|Equatorial Guinea|\n",
      "|       Costa Rica|\n",
      "|          Senegal|\n",
      "+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 전체 쿼리를 데이터베이스에 전달해 결과를 DataFrame으로 받을 수도 있다.\n",
    "# 테이블에서 'DEST_COUNTRY_NAME' 열의 고유한 값을 모두 선택하여 반환\n",
    "pushdownQuery = \"\"\"(SELECT DISTINCT(DEST_COUNTRY_NAME) FROM flight_info)\n",
    "  AS flight_info\"\"\"\n",
    "\n",
    "dbDataFrame = (\n",
    "    spark.read.format(\"jdbc\")\n",
    "    .option(\"url\", url)\n",
    "    .option(\"dbtable\", pushdownQuery)\n",
    "    .option(\"driver\", driver)\n",
    "    .load()\n",
    ")\n",
    "\n",
    "dbDataFrame.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7694a111-4088-486e-88c9-59eac98d5639",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Scan JDBCRelation((SELECT DISTINCT(DEST_COUNTRY_NAME) FROM flight_info)\n",
      "  AS flight_info) [numPartitions=1] [DEST_COUNTRY_NAME#223] PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string>\n"
     ]
    }
   ],
   "source": [
    "dbDataFrame.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9254c6-49a2-4da3-b82c-bb11c86b271c",
   "metadata": {},
   "source": [
    "### 데이터베이스 병렬로 읽기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "26e91320-1677-4c14-a44d-a2eac3f10c83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dbDataFrame = (\n",
    "    spark.read.format(\"jdbc\")\n",
    "    .option(\"url\", url)\n",
    "    .option(\"dbtable\", tablename)\n",
    "    .option(\"driver\", driver)\n",
    "    .option(\"numPartitions\", 10)\n",
    "    .load()\n",
    ")  # 읽기 및 쓰기용 동시 작업 수를 제한할 수 있는 최대 파티션 수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89081b9c-d7e8-44bb-bf58-4c82df9c30ab",
   "metadata": {},
   "source": [
    "### SQL 데이터베이스 쓰기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d2ac9d7e-01ad-4419-b758-e9c852799634",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-03-21 09:54:35 WARN  JdbcUtils:66 - Requested isolation level 1 is not supported; falling back to default isolation level 8\n"
     ]
    }
   ],
   "source": [
    "props = {\"driver\": \"org.sqlite.JDBC\"}\n",
    "newPath = \"jdbc:sqlite://tmp/my-sqlite.db\"\n",
    "csvFile.write.jdbc(newPath, tablename, mode=\"overwrite\", properties=props)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fe0213e4-e29f-431d-b6b4-886a4242092b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "255"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.jdbc(newPath, tablename, properties=props).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b8ba69ec-90d1-41dd-9966-cb8007e1c927",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-03-21 09:56:53 WARN  JdbcUtils:66 - Requested isolation level 1 is not supported; falling back to default isolation level 8\n"
     ]
    }
   ],
   "source": [
    "csvFile.write.jdbc(newPath, tablename, mode=\"append\", properties=props)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "81c74561-5ac9-4b17-9865-5b83a71d5b09",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "510"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.jdbc(newPath, tablename, properties=props).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40ec27f-999e-40d2-83dc-d1cdede13ec7",
   "metadata": {},
   "source": [
    "## 텍스트 파일"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e44037-c306-410a-bf0b-d6d13d6e4b3c",
   "metadata": {},
   "source": [
    "### 텍스트 파일 쓰기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03689f7c-7fb3-4af3-a131-31f4e5c34d79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 문자열 컬럼이 하나만 존재해야 한다.\n",
    "csvFile.select(\"DEST_COUNTRY_NAME\").write.text(\"/tmp/simple-text-file.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0632f237-9262-49c4-b956-7762fcd7f40b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 데이터를 저장할 때 파티셔닝 작업을 수행하면 더 많은 컬럼을 저장할 수 있다.\n",
    "csvFile.limit(10).select(\"DEST_COUNTRY_NAME\", \"count\").write.partitionBy(\"count\").text(\n",
    "    \"/tmp/five-csv-files2py.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2282f00f-cf4b-429c-ab3d-bf26cd43c011",
   "metadata": {},
   "source": [
    "## 고급 I / O 개념  \n",
    "  \n",
    "  쓰기 작업 전에 파티션 수를 조절함으로써 병렬로 처리할 파일 수를 제어할 수 있다.  \n",
    "  또한 버켓팅과 파티셔닝을 조절함으로써 데이터의 저장 구조를 제어할 수 있다.  \n",
    "    \n",
    "  특정 파일 포맷은 기본적으로 분할을 지원하므로, 스파크에서 전체 파일이 아닌 쿼리에 필요한 부분만 읽을 수 있다.  \n",
    "  HDFS 같은 시스템을 사용한다면 분할된 파일을 여러 블록으로 나누어 분산 저장하기 때문에 훨씬 더 최적화할 수 있다.  \n",
    "  추천하는 파일 포맷과 압축 방식은 '파케이' 파일 포맷과 'GZIP' 압축 방식이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927df662-761d-4baa-b335-2f2cb8eebc90",
   "metadata": {},
   "source": [
    "### 병렬로 데이터 읽기  \n",
    "  \n",
    "  여러 익스큐터가 같은 파일을 동시에 읽을 수는 없지만 여러 파일을 동시에 읽을 수는 있다.  \n",
    "  폴더의 개별 파일은 DataFrame의 파티션이 된다.  \n",
    "  따라서 사용 가능한 익스큐터를 이용해 병렬로 파일을 읽는다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b71a888-9ecd-41d7-adf1-0b7243bd5502",
   "metadata": {},
   "source": [
    "### 병렬로 데이터 쓰기  \n",
    "  \n",
    "  파일 또는 데이터의 수는 데이터를 쓰는 시점에 DataFrame이 가진 파티션 수에 따라 달라질 수 있다.  \n",
    "  기본적으로 데이터 파티션 당 하나의 파일  \n",
    "  다음 코드는 폴더 안에 5개의 파일을 생성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b05c5f5-d71f-4d09-bc86-79db0150a0f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "csvFile.repartition(5).write.format(\"csv\").save(\"/tmp/multiple.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "42d1aa2d-0b8d-4f96-becb-72532cb1b0b4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "part-00000-2d4e5eaa-e056-4c1a-9a4c-fecf7ecc415e-c000.csv\n",
      "part-00001-2d4e5eaa-e056-4c1a-9a4c-fecf7ecc415e-c000.csv\n",
      "part-00002-2d4e5eaa-e056-4c1a-9a4c-fecf7ecc415e-c000.csv\n",
      "part-00003-2d4e5eaa-e056-4c1a-9a4c-fecf7ecc415e-c000.csv\n",
      "part-00004-2d4e5eaa-e056-4c1a-9a4c-fecf7ecc415e-c000.csv\n",
      "_SUCCESS\n"
     ]
    }
   ],
   "source": [
    "%ls /tmp/multiple.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c4f700-bfc0-40e0-a3d6-71d12997c099",
   "metadata": {},
   "source": [
    "### 파티셔닝  \n",
    "  \n",
    "  파티셔닝: 어떤 데이터를 어디에 저장할 것인지 제어할 수 있는 기능  \n",
    "  데이터를 읽을 때 전체 데이터셋을 스캔하지 않고 필요한 컬럼의 데이터만 읽을 수 있게 한다.  \n",
    "  모든 파일 기반의 데이터소스에서 지원"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "897c23d9-623e-4942-8b36-1d7e6dec2039",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "csvFile.limit(10).write.mode(\"overwrite\").partitionBy(\"DEST_COUNTRY_NAME\").save(\n",
    "    \"/tmp/partitioned-files.parquet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1332bf41-1788-4c0c-a3b8-eb45fdf608e9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34m'DEST_COUNTRY_NAME=Costa Rica'\u001b[0m/         \u001b[01;34m'DEST_COUNTRY_NAME=Senegal'\u001b[0m/\n",
      "\u001b[01;34m'DEST_COUNTRY_NAME=Egypt'\u001b[0m/              \u001b[01;34m'DEST_COUNTRY_NAME=United States'\u001b[0m/\n",
      "\u001b[01;34m'DEST_COUNTRY_NAME=Equatorial Guinea'\u001b[0m/   _SUCCESS\n"
     ]
    }
   ],
   "source": [
    "%ls /tmp/partitioned-files.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6ccaa6cf-0c9e-4276-a88c-fe893b96a105",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "part-00000-99767eca-0928-4e6a-9ca3-010f42e63ad6.c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "%ls /tmp/partitioned-files.parquet/DEST_COUNTRY_NAME=Senegal/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4c9a50-88ce-4ab0-a0d3-0c6c8d9de7e5",
   "metadata": {},
   "source": [
    "### 버켓팅  \n",
    "  \n",
    "  버켓팅: 각 파일에 저장된 데이터를 제어할 수 있는 또 다른 파일 조직화 기법  \n",
    "  동일한 버킷 ID를 가진 데이터가 하나의 물리적 파티션에 모두 모여 있기 때문에 데이터를 읽을 때 셔플을 피할 수 있다.  \n",
    "    \n",
    "  특정 컬럼을 파티셔닝하면 수억 개의 디렉터리가 만들어질 수도 있으므로 데이터를 버켓팅할 수 있는 방법을 찾아야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b1b3fc40-ff39-4f33-97b9-81139c9e9ae5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 버켓팅은 스파크 관리 테이블에서만 사용할 수 있다.\n",
    "numberBuckets = 10\n",
    "columnToBucketBy = \"count\"\n",
    "\n",
    "csvFile.write.format(\"parquet\").mode(\"overwrite\").bucketBy(\n",
    "    numberBuckets, columnToBucketBy\n",
    ").saveAsTable(\"bucketedFiles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5b2a50c2-f5c1-4d93-aa1a-f05875da7abe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "part-00000-39450f84-30cb-47de-ad6b-8a4ec6c670a7_00000.c000.snappy.parquet\n",
      "part-00000-39450f84-30cb-47de-ad6b-8a4ec6c670a7_00001.c000.snappy.parquet\n",
      "part-00000-39450f84-30cb-47de-ad6b-8a4ec6c670a7_00002.c000.snappy.parquet\n",
      "part-00000-39450f84-30cb-47de-ad6b-8a4ec6c670a7_00003.c000.snappy.parquet\n",
      "part-00000-39450f84-30cb-47de-ad6b-8a4ec6c670a7_00004.c000.snappy.parquet\n",
      "part-00000-39450f84-30cb-47de-ad6b-8a4ec6c670a7_00005.c000.snappy.parquet\n",
      "part-00000-39450f84-30cb-47de-ad6b-8a4ec6c670a7_00006.c000.snappy.parquet\n",
      "part-00000-39450f84-30cb-47de-ad6b-8a4ec6c670a7_00007.c000.snappy.parquet\n",
      "part-00000-39450f84-30cb-47de-ad6b-8a4ec6c670a7_00008.c000.snappy.parquet\n",
      "part-00000-39450f84-30cb-47de-ad6b-8a4ec6c670a7_00009.c000.snappy.parquet\n",
      "_SUCCESS\n"
     ]
    }
   ],
   "source": [
    "%ls spark-warehouse/bucketedfiles/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeea1f4c-7522-4d74-b8fb-19e61801f21c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 파일 크기 관리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f4ca428f-7930-4506-9fef-1156b773386a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.readwriter.DataFrameWriter at 0xffff9870bcd0>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 파일당 최대 5,000 개의 로우를 포함하도록 설정\n",
    "df.write.option(\"maxRecordPerFile\", 5000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
