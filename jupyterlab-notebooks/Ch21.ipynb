{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "402ead67-ae8e-4050-a842-42d92701295e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/04/02 16:06:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    from pyspark.sql import SparkSession\n",
    "\n",
    "    spark = (\n",
    "        SparkSession.builder.master(\"local\")\n",
    "        .appName(\"Word Count\")\n",
    "        .config(\"spark.some.config.option\", \"some-value\")\n",
    "        .getOrCreate()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5729742c-1eb1-4692-935a-bbfbd63cb878",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "static = spark.read.json(\"/data/activity-data\")\n",
    "dataSchema = static.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3cc91f04-dee4-4724-8cc3-864ee0de5822",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('Arrival_Time', LongType(), True), StructField('Creation_Time', LongType(), True), StructField('Device', StringType(), True), StructField('Index', LongType(), True), StructField('Model', StringType(), True), StructField('User', StringType(), True), StructField('gt', StringType(), True), StructField('x', DoubleType(), True), StructField('y', DoubleType(), True), StructField('z', DoubleType(), True)])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3394f170-5fb6-4738-98f1-a74d7b5d14df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------------+--------+-----+------+----+-----+------------+------------+------------+\n",
      "| Arrival_Time|      Creation_Time|  Device|Index| Model|User|   gt|           x|           y|           z|\n",
      "+-------------+-------------------+--------+-----+------+----+-----+------------+------------+------------+\n",
      "|1424686735090|1424686733090638193|nexus4_1|   18|nexus4|   g|stand| 3.356934E-4|-5.645752E-4|-0.018814087|\n",
      "|1424686735292|1424688581345918092|nexus4_2|   66|nexus4|   g|stand|-0.005722046| 0.029083252| 0.005569458|\n",
      "|1424686735500|1424686733498505625|nexus4_1|   99|nexus4|   g|stand|   0.0078125|-0.017654419| 0.010025024|\n",
      "|1424686735691|1424688581745026978|nexus4_2|  145|nexus4|   g|stand|-3.814697E-4|   0.0184021|-0.013656616|\n",
      "|1424686735890|1424688581945252808|nexus4_2|  185|nexus4|   g|stand|-3.814697E-4|-0.031799316| -0.00831604|\n",
      "+-------------+-------------------+--------+-----+------+----+-----+------------+------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "static.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9516d1ab-6f38-476d-ab99-1e799c094c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming = spark.readStream.schema(dataSchema).option(\"maxFilesPerTrigger\", 1)\\\n",
    "    .json(\"/data/activity-data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "423232ee-fdb9-4457-9387-4e3d1b52c45d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[gt: string, count: bigint]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activityCounts = streaming.groupBy(\"gt\").count()\n",
    "activityCounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c87823c4-70d2-4efe-a24b-d8afeb9b9f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0082878d-a0e4-4619-ac31-100cc399b99e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/02 16:07:50 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-29420ad7-4e11-4ebe-bd95-cf363ed4a0c5. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "24/04/02 16:07:50 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    }
   ],
   "source": [
    "# 메모리 싱크\n",
    "\n",
    "activityQuery = activityCounts.writeStream.queryName(\"activity_counts\")\\\n",
    "    .format(\"memory\").outputMode(\"complete\")\\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "972376e6-f8a5-48c0-b656-8606dde7c2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 코드 실행시 백그라운드에서 스트리밍 연산 시작됨.\n",
    "# 그런데 실행이 끝나지 않아서 주피터가 다른 셀 실행 못함 .???\n",
    "# -> timeout=## 파라미터로 설정 가능\n",
    "\n",
    "# activityQuery.awaitTermination(timeout=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a1534e6-b7eb-445e-be5a-8319b9d6db7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+\n",
      "|        gt| count|\n",
      "+----------+------+\n",
      "|       sit|418471|\n",
      "|     stand|387084|\n",
      "|stairsdown|318313|\n",
      "|      walk|450702|\n",
      "|  stairsup|355543|\n",
      "|      null|355192|\n",
      "|      bike|367109|\n",
      "+----------+------+\n",
      "\n",
      "+----------+------+\n",
      "|        gt| count|\n",
      "+----------+------+\n",
      "|       sit|467703|\n",
      "|     stand|432616|\n",
      "|stairsdown|355759|\n",
      "|      walk|503722|\n",
      "|  stairsup|397393|\n",
      "|      null|396980|\n",
      "|      bike|410297|\n",
      "+----------+------+\n",
      "\n",
      "+----------+------+\n",
      "|        gt| count|\n",
      "+----------+------+\n",
      "|       sit|529241|\n",
      "|     stand|489536|\n",
      "|stairsdown|402587|\n",
      "|      walk|569997|\n",
      "|  stairsup|449685|\n",
      "|      null|449217|\n",
      "|      bike|464277|\n",
      "+----------+------+\n",
      "\n",
      "+----------+------+\n",
      "|        gt| count|\n",
      "+----------+------+\n",
      "|       sit|578473|\n",
      "|     stand|535076|\n",
      "|stairsdown|440060|\n",
      "|      walk|623016|\n",
      "|  stairsup|491513|\n",
      "|      null|491001|\n",
      "|      bike|507457|\n",
      "+----------+------+\n",
      "\n",
      "+----------+------+\n",
      "|        gt| count|\n",
      "+----------+------+\n",
      "|       sit|627709|\n",
      "|     stand|580615|\n",
      "|stairsdown|477528|\n",
      "|      walk|676032|\n",
      "|  stairsup|533348|\n",
      "|      null|532780|\n",
      "|      bike|550637|\n",
      "+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from time import sleep\n",
    "\n",
    "# 10초 후에 스트리밍 쿼리 종료\n",
    "activityQuery.awaitTermination(timeout=10)\n",
    "\n",
    "for x in range(5):\n",
    "    spark.sql(\"SELECT * FROM activity_counts\").show()\n",
    "    sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2696cf45-0f1b-4114-96ff-ccb1bae65feb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/02 16:08:05 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-fe0b2a40-f761-4b02-beee-a5abe1eb58fc. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "24/04/02 16:08:05 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "simpleTransform = streaming.withColumn(\"stairs\", expr(\"gt like '%stairs%'\"))\\\n",
    "    .where(\"stairs\")\\\n",
    "    .where(\"gt is not null\")\\\n",
    "    .select(\"gt\", \"model\", \"arrival_time\", \"creation_time\")\\\n",
    "    .writeStream\\\n",
    "    .queryName(\"simple_transform\")\\\n",
    "    .format(\"memory\")\\\n",
    "    .outputMode(\"append\")\\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8f1e063f-0e5e-48f7-a37c-329853e08569",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/02 16:11:26 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-146a11c6-f6ec-48c2-a7b4-bd6047e48bcd. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "24/04/02 16:11:26 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    }
   ],
   "source": [
    "deviceModelStats = streaming.cube(\"gt\", \"model\").avg()\\\n",
    "    .drop(\"avg(Arrival_time)\")\\\n",
    "    .drop(\"avg(Creation_Time)\")\\\n",
    "    .drop(\"avg(Index)\")\\\n",
    "    .writeStream.queryName(\"device_counts\").format(\"memory\")\\\n",
    "    .outputMode(\"complete\")\\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a39f3618-c2c4-4656-8df4-cf16fc494b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+--------------------+--------------------+--------------------+\n",
      "|      gt| model|              avg(x)|              avg(y)|              avg(z)|\n",
      "+--------+------+--------------------+--------------------+--------------------+\n",
      "|     sit|  null|-5.49433244039590...|2.791446281700068E-4|-2.33994461689890...|\n",
      "|    walk|nexus4|-0.00390116006094368|0.001052508689953...|-6.95435553042992...|\n",
      "|    walk|  null|-0.00390116006094368|0.001052508689953...|-6.95435553042992...|\n",
      "|stairsup|  null|-0.02479965287771635|-0.00800392344379...|  -0.100340884150604|\n",
      "|   stand|  null|-3.11082189691724...|3.218461665975318E-4|2.141300040636475...|\n",
      "+--------+------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM device_counts\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1db311d0-40e8-4289-99b2-386ec3d9b356",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/02 16:11:33 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-7bd55588-6aae-4823-9309-2abf98f71db9. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "24/04/02 16:11:33 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "historicalAgg = static.groupBy(\"gt\", \"model\").avg()\n",
    "\n",
    "deviceModelStats = streaming.drop(\"Arrival_Time\", \"Creation_Time\", \"Index\")\\\n",
    "    .cube(\"gt\", \"model\").avg()\\\n",
    "    .join(historicalAgg, [\"gt\", \"model\"])\\\n",
    "    .writeStream.queryName(\"device_counts\").format(\"memory\")\\\n",
    "    .outputMode(\"complete\")\\\n",
    "    .start()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
